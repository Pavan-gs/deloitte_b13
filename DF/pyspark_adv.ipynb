{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'ID':[1,2,3],'Dep':['IT','HR','FI'],'Sal':[12000,78906,62427]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1=pickle.dumps(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pickle.loads(d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!whereis spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init('/usr/local/spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MemoryManagementDemo\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.6\") \\\n",
    "    .config(\"spark.memory.storageFraction\", \"0.5\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.range(0, 10000000)  # Large DataFrame\n",
    "df = df.withColumn(\"square\", df[\"id\"] * df[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.cache()  # Stored in storage memory/off-heap\n",
    "df.count()  # Trigger caching (materialize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "shuffled = df.repartition(200).groupBy(col(\"id\") % 10).count()\n",
    "shuffled.explain(True)\n",
    "shuffled.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.range(0, 10000000)\n",
    "joined = df.join(df2, \"id\")\n",
    "joined.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.memory.offHeap.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.memory.offHeap.size\", \"512m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Create a sample DataFrame\n",
    "df = spark.range(0, 100).withColumnRenamed(\"id\", \"value\")\n",
    "\n",
    "# Define a UDF that labels even/odd\n",
    "def label_even_odd(n):\n",
    "    return \"even\" if n % 2 == 0 else \"odd\"\n",
    "\n",
    "label_udf = udf(label_even_odd, StringType())\n",
    "\n",
    "# Apply UDF\n",
    "df_with_udf = df.withColumn(\"label\", label_udf(df[\"value\"]))\n",
    "#df_with_udf.show()\n",
    "df_with_udf.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "df_fast = df.withColumn(\n",
    "    \"label\", when(df[\"value\"] % 2 == 0, \"even\").otherwise(\"odd\")\n",
    ")\n",
    "df_fast.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buckets\n",
    "df.write.bucketBy(4, \"value\").sortBy(\"value\").saveAsTable(\"bucketed_table\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "explode() → Flattens array to multiple rows\n",
    "\n",
    "split() → Converts comma string to array\n",
    "\n",
    "when() + otherwise() → SQL CASE-WHEN equivalent\n",
    "\n",
    ".explain(\"formatted\") → Shows  plan in formatted mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, when, col, split\n",
    "\n",
    "spark = SparkSession.builder.appName(\"TransformDemo\").getOrCreate()\n",
    "\n",
    "# Sample Data\n",
    "data = [\n",
    "    (\"John\", \"A,B,C\", 85),\n",
    "    (\"Sara\", \"A\", 92),\n",
    "    (\"Mike\", \"B,C\", 70)\n",
    "]\n",
    "df = spark.createDataFrame(data, [\"name\", \"subjects\", \"score\"])\n",
    "\n",
    "# 1️⃣ Using explode after splitting comma-separated values\n",
    "df1 = df.withColumn(\"subject\", explode(split(col(\"subjects\"), \",\")))\n",
    "\n",
    "# 2️⃣ Using 'when' to add a status column\n",
    "df2 = df1.withColumn(\"status\", when(col(\"score\") >= 80, \"Passed\").otherwise(\"Retest\"))\n",
    "\n",
    "df2.show(truncate=False)\n",
    "df2.explain(\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cached = df2.cache()\n",
    "df_cached.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salting Technique\n",
    "from pyspark.sql.functions import rand, concat_ws\n",
    "df1 = df1.withColumn(\"salt\", (rand() * 10).cast(\"int\"))\n",
    "df2 = df2.withColumn(\"salt\", (rand() * 10).cast(\"int\"))\n",
    "df_joined = df1.join(df2, [\"key\", \"salt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df = spark.read.option(\"header\", \"true\").csv('churn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df.write.parquet(\"churn1.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parquet: Save and query with predicate pushdown\n",
    "\n",
    "parquet_df = spark.read.parquet(\"churn1.parquet\")\n",
    "parquet_df.filter(col(\"Age\") > 30)\n",
    "parquet_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spark.conf.get(\"spark.sql.parquet.filterPushdown\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, split\n",
    "\n",
    "# Read streaming text files\n",
    "lines = spark.readStream.format(\"text\") \\\n",
    "    .option(\"path\", \"/mnt/input/streaming/\") \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .load()\n",
    "\n",
    "# Split into words\n",
    "words = lines.select(explode(split(lines.value, \" \")).alias(\"word\"))\n",
    "\n",
    "# Word count\n",
    "wordCounts = words.groupBy(\"word\").count()\n",
    "\n",
    "# Output to console\n",
    "query = wordCounts.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"checkpointLocation\", \"/mnt/checkpoints/wordcount/\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ".withWatermark() allows Spark to:\n",
    "\n",
    "Keep old state up to 10 mins\n",
    "\n",
    "Drop events arriving too late\n",
    "\n",
    "Avoid memory overload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window\n",
    "\n",
    "# Assume data has columns: value, timestamp\n",
    "\n",
    "aggregated = df.withWatermark(\"timestamp\", \"10 minutes\") \\\n",
    "    .groupBy(window(\"timestamp\", \"5 minutes\"), \"value\") \\\n",
    "    .count()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Options:\n",
    "processingTime: micro-batch interval\n",
    "once: run one batch only\n",
    "continuous: low-latency mode (experimental)\n",
    "\n",
    ".writeStream.trigger(processingTime='30 seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Saving DataFrame as Parquet\n",
    "df.write.mode(\"overwrite\").parquet(\"/tmp/users_parquet\")\n",
    "\n",
    "# 2. Saving as Delta\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/users_delta\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Delta-specific operations\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "delta_table = DeltaTable.forPath(spark, \"/tmp/users_delta\")\n",
    "\n",
    "# Time Travel: View previous version\n",
    "spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"/tmp/users_delta\").show\n",
    "\n",
    "'''spark.read.format(\"delta\") \\\n",
    "    .option(\"timestampAsOf\", \"2024-06-01 10:00:00\") \\\n",
    "    .load(\"/path/to/table\")\n",
    "'''\n",
    "\n",
    "# Merge example (Upsert)\n",
    "updates_df = ...\n",
    "delta_table.alias(\"t\").merge(\n",
    "    updates_df.alias(\"s\"),\n",
    "    \"t.id = s.id\"\n",
    ").whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check physical plan and predicate pushdown\n",
    "df.filter(\"age > 30\").explain(\"formatted\")\n",
    "\n",
    "# Inspect underlying files\n",
    "df.inputFiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "@dlt.table\n",
    "def bronze_orders():\n",
    "    return spark.read.format(\"json\").load(\"/mnt/raw/orders\")\n",
    "\n",
    "@dlt.table\n",
    "def silver_orders():\n",
    "    return dlt.read(\"bronze_orders\").filter(col(\"status\") == \"completed\")\n",
    "\n",
    "@dlt.table\n",
    "def gold_order_totals():\n",
    "    return dlt.read(\"silver_orders\") \\\n",
    "              .groupBy(\"customer_id\") \\\n",
    "              .sum(\"amount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
