{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Dataframes from RDD programmatically\n",
    "\n",
    "Import findspark and initiate.\n",
    "Then import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/usr/local/spark')\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Python Spark SQL example\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an RDD from the structured text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clines = sc.textFile(\"customers.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clines.take(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import types from sql to be able to create StructTypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfields = clines.map(lambda l: l.split(\"\\t\"))\n",
    "customers = cfields.map(lambda p: (p[0], p[1], p[2], p[3], p[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The schema encoded in a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "schemaString = \"cid cname ccity cstate czip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccolumns = [StructField(column_name, StringType(), True) for column_name in schemaString.split()]\n",
    "schema = StructType(ccolumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ccolumns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the schema to the RDD to create the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "customerDF = spark.createDataFrame(customers, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "customerDF.write.parquet('mycust')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customerDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust = spark.read.parquet('mycust')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customerDF.select(\"cname\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customerDF.select(customerDF['cname'], customerDF['ccity']).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customerDF.filter(customerDF['cstate'] == 'CA').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------+---------------+------+-----+\n",
      "|  cid|           cname|          ccity|cstate| czip|\n",
      "+-----+----------------+---------------+------+-----+\n",
      "| 5577|      Mary Smith|        Modesto|    CA|95350|\n",
      "| 1745|      Mary Smith|Rowland Heights|    CA|91748|\n",
      "|11444|Kathleen Patrick|      San Diego|    CA|92109|\n",
      "| 8846|    Thomas Smith|          Indio|    CA|92201|\n",
      "| 6237|  Bobby Anderson|       El Cajon|    CA|92020|\n",
      "| 4085|       Mary Carr|  Panorama City|    CA|91402|\n",
      "| 8705|  Patricia Smith|       Stockton|    CA|95207|\n",
      "| 3669|       Mary Soto| San Bernardino|    CA|92410|\n",
      "| 6101|      Mary Smith|    Los Angeles|    CA|90033|\n",
      "|11697|  Jessica Thomas|  Laguna Niguel|    CA|92677|\n",
      "| 1295|   Theresa Lopez|       Winnetka|    CA|91306|\n",
      "| 4814|     Paul Suarez|    Simi Valley|    CA|93065|\n",
      "| 8530|   William Smith|       Highland|    CA|92346|\n",
      "| 3846|    Ronald Lewis|        Ontario|    CA|91764|\n",
      "|10476|     John Hodges|       Cerritos|    CA|90703|\n",
      "|10243|  Donna Anderson|    Los Angeles|    CA|90034|\n",
      "|11595|   Zachary Jones|        Modesto|    CA|95355|\n",
      "|  847|  Jerry Ferguson|      San Diego|    CA|92102|\n",
      "| 3440|    Mary Edwards|        Salinas|    CA|93905|\n",
      "| 3400|     Frank Lewis|  Moreno Valley|    CA|92557|\n",
      "+-----+----------------+---------------+------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "*FileScan parquet [cid#35,cname#36,ccity#37,cstate#38,czip#39] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/home/hduser/del_hyd/DF/mycust], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<cid:string,cname:string,ccity:string,cstate:string,czip:string>\n"
     ]
    }
   ],
   "source": [
    "cust.filter(cust['cstate'] == 'CA').show()\n",
    "cust.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customerDF.groupBy(\"cstate\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a temp view so that SQL queries can be run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customerDF.createOrReplaceTempView(\"customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cStateCount50 = spark.sql(\"SELECT cstate, count(*) as sttcount FROM customers GROUP BY cstate HAVING sttcount>=50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cStateCount50.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cStateCount50.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(cStateCount50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize([1,2,3,4,5,6,7,8,9,10,20,40,30,88,78,66,77,44,84,22], numSlices=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1.coalesce(2).glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cannot be used to increase the number of partitions\n",
    "rdd1.coalesce(6).glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1.repartition(2).glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
